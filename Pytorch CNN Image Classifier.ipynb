{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify transforms\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.6, contrast=0.7, saturation=1, hue=0.4),\n",
    "    transforms.RandomResizedCrop((224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transformations = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in each dataset and apply transformations specified above\n",
    "train_set = datasets.ImageFolder(\"flower_data/train\", transform = train_transformations)\n",
    "val_set = datasets.ImageFolder(\"flower_data/valid\", transform = val_transformations)\n",
    "\n",
    "#Put into a Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size =32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dictionary for mapping label to name of specied\n",
    "lab_to_name = {\"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\", \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\", \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\", \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\", \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\", \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\", \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\", \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\", \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\", \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\", \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\", \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\", \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\", \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\", \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\", \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\", \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\", \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\", \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\", \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\", \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get pretrained model\n",
    "model = models.densenet161(pretrained=True)\n",
    "\n",
    "#Turn off training for their parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Create new classifier for model\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2208, 1024)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(1024, 512)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(512, 102)),\n",
    "            ('output', nn.LogSoftmax(dim=1))\n",
    "        ]))\n",
    "\n",
    "model.classifier = classifier\n",
    "\n",
    "#load parameters that I trained\n",
    "state_dict = torch.load('model.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the device available to use (GPU is faster but if not, CPU could work)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Set the error function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "#Set the optimizer function\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "#Move model to the device specified above\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "valid_loss_min = np.inf\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    #Start training the model\n",
    "    #Set model to train mode\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #Clear optimizers\n",
    "        optimizer.zero_grad()\n",
    "        #Forward pass\n",
    "        output = model.forward(inputs)\n",
    "        #Loss\n",
    "        loss = criterion(output, labels)\n",
    "        #Calculate gradients (backpropogation)\n",
    "        loss.backward()\n",
    "        #Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        #Add the loss to the training set's running loss\n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        \n",
    "        counter += 1\n",
    "        print(counter, \"/\", len(train_loader))\n",
    "        \n",
    "    #Evaluate the model\n",
    "    #Set model to evaluation mode\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    #Tell torch not to calculate grads\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #Forward pass\n",
    "            output = model.forward(inputs)\n",
    "            #Calculate Loss\n",
    "            valloss = criterion(output, labels)\n",
    "            #Add loss to the validation set's running loss\n",
    "            val_loss += valloss.item()*inputs.size(0)\n",
    "            \n",
    "            #Since our model outputs a LogSoftmax, find the real percentages\n",
    "            ps = torch.exp(output)\n",
    "            #Get the top class of the output\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            #See how many of the classes were correct\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            #Calculate the mean (get the accuracy for this batch) and add it to the \n",
    "            #running accuracy for this epoch\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "            count += 1\n",
    "            print(count, \"/\", len(val_loader))\n",
    "    \n",
    "    #Get the average loss for the epoch\n",
    "    train_loss = running_loss/len(train_loader.dataset)\n",
    "    valid_loss = val_loss/len(val_loader.dataset)\n",
    "    #Print out data\n",
    "    print('Accuracy: ', accuracy/len(val_loader))\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "    \n",
    "    #Save the model if the validation loss this time is less than the validation loss last time\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
